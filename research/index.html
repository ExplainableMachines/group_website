<!DOCTYPE html>
<html lang="en" data-dark="false">
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!--
  put your analytics (e.g. Google Analytics) tracking code here
-->

  <!--
  put your search engine verification (e.g. Google Search Console) tag here
-->

  


























<meta name="viewport" content="width=device-width, initial-scale=1">

<title>Research | Explainable Machine Learning Group</title>

<link rel="icon" href="/images/icon.png">

<meta name="title" content="Research">
<meta name="description" content="EML. This is the home of the Explainable Machine Learning research group of the Department for Computer Vision and Machine Learning of the Max Planck Institute for Informatics. Here, we provide an overview of our ongoing and past work around Explainability in Machine Learning, including generation of explanations for black box models, inherently interpretable neural network approaches, and their applications.">

<meta property="og:title" content="Research">
<meta property="og:site_title" content="Explainable Machine Learning Group">
<meta property="og:description" content="EML. This is the home of the Explainable Machine Learning research group of the Department for Computer Vision and Machine Learning of the Max Planck Institute for Informatics. Here, we provide an overview of our ongoing and past work around Explainability in Machine Learning, including generation of explanations for black box models, inherently interpretable neural network approaches, and their applications.">
<meta property="og:url" content="">
<meta property="og:image" content="">
<meta property="og:locale" content="en_US">

<meta property="twitter:title" content="Research">
<meta property="twitter:description" content="EML. This is the home of the Explainable Machine Learning research group of the Department for Computer Vision and Machine Learning of the Max Planck Institute for Informatics. Here, we provide an overview of our ongoing and past work around Explainability in Machine Learning, including generation of explanations for black box models, inherently interpretable neural network approaches, and their applications.">
<meta property="twitter:url" content="">
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="">


  <meta property="og:type" content="website">


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "WebSite",
    
    "name": "Research",
    "description": "EML. This is the home of the Explainable Machine Learning research group of the Department for Computer Vision and Machine Learning of the Max Planck Institute for Informatics. Here, we provide an overview of our ongoing and past work around Explainability in Machine Learning, including generation of explanations for black box models, inherently interpretable neural network approaches, and their applications.",
    "headline": "Research",
    "publisher": {
      "@type": "Organization",
      "logo": { "@type": "ImageObject", "url": "/images/icon.png" }
    },
    "url": ""
  }
</script>

<link rel="alternate" type="application/rss+xml" href="/feed.xml">

  <!-- Google Fonts -->
<!-- automatically get url from fonts used in theme file -->

<link rel="preconnect" href="https://fonts.gstatic.com">
<link href="https://fonts.googleapis.com/css2?display=swap&&family=Barlow:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600&amp;family=Roboto+Mono:ital,wght@0,200;0,400;0,500;0,600;1,200;1,400;1,500;1,600" rel="stylesheet">

<!-- Font Awesome icons (load asynchronously due to size) -->

<link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="preload" as="style" onload="this.onload = null; this.rel = 'stylesheet';">
<noscript>
  <link href="https://use.fontawesome.com/releases/v6.3.0/css/all.css" rel="stylesheet">
</noscript>

  <!-- third party styles -->
<!-- https://stylishthemes.github.io/Syntax-Themes/pygments/ -->
<link href="https://cdn.jsdelivr.net/gh/StylishThemes/Syntax-Themes/pygments/css-github/pygments-tomorrow-night-eighties.css" rel="stylesheet">

<!-- include all sass in styles folder -->


  
    <link href="/_styles/-theme.css" rel="stylesheet">
  

  
    <link href="/_styles/alert.css" rel="stylesheet">
  

  
    <link href="/_styles/all.css" rel="stylesheet">
  

  
    <link href="/_styles/anchor.css" rel="stylesheet">
  

  
    <link href="/_styles/background.css" rel="stylesheet">
  

  
    <link href="/_styles/body.css" rel="stylesheet">
  

  
    <link href="/_styles/bold.css" rel="stylesheet">
  

  
    <link href="/_styles/button.css" rel="stylesheet">
  

  
    <link href="/_styles/card.css" rel="stylesheet">
  

  
    <link href="/_styles/checkbox.css" rel="stylesheet">
  

  
    <link href="/_styles/citation.css" rel="stylesheet">
  

  
    <link href="/_styles/code.css" rel="stylesheet">
  

  
    <link href="/_styles/cols.css" rel="stylesheet">
  

  
    <link href="/_styles/dark-toggle.css" rel="stylesheet">
  

  
    <link href="/_styles/feature.css" rel="stylesheet">
  

  
    <link href="/_styles/figure.css" rel="stylesheet">
  

  
    <link href="/_styles/float.css" rel="stylesheet">
  

  
    <link href="/_styles/font.css" rel="stylesheet">
  

  
    <link href="/_styles/footer.css" rel="stylesheet">
  

  
    <link href="/_styles/form.css" rel="stylesheet">
  

  
    <link href="/_styles/grid.css" rel="stylesheet">
  

  
    <link href="/_styles/header.css" rel="stylesheet">
  

  
    <link href="/_styles/heading.css" rel="stylesheet">
  

  
    <link href="/_styles/highlight.css" rel="stylesheet">
  

  
    <link href="/_styles/icon.css" rel="stylesheet">
  

  
    <link href="/_styles/image.css" rel="stylesheet">
  

  
    <link href="/_styles/link.css" rel="stylesheet">
  

  
    <link href="/_styles/list.css" rel="stylesheet">
  

  
    <link href="/_styles/main.css" rel="stylesheet">
  

  
    <link href="/_styles/paragraph.css" rel="stylesheet">
  

  
    <link href="/_styles/portrait.css" rel="stylesheet">
  

  
    <link href="/_styles/post-excerpt.css" rel="stylesheet">
  

  
    <link href="/_styles/post-info.css" rel="stylesheet">
  

  
    <link href="/_styles/post-nav.css" rel="stylesheet">
  

  
    <link href="/_styles/quote.css" rel="stylesheet">
  

  
    <link href="/_styles/rule.css" rel="stylesheet">
  

  
    <link href="/_styles/search-box.css" rel="stylesheet">
  

  
    <link href="/_styles/search-info.css" rel="stylesheet">
  

  
    <link href="/_styles/section.css" rel="stylesheet">
  

  
    <link href="/_styles/table.css" rel="stylesheet">
  

  
    <link href="/_styles/tags.css" rel="stylesheet">
  

  
    <link href="/_styles/textbox.css" rel="stylesheet">
  

  
    <link href="/_styles/tooltip.css" rel="stylesheet">
  

  
    <link href="/_styles/util.css" rel="stylesheet">
  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  


<!-- include all css in styles folder -->



  <!-- third party scripts -->
<script src="https://unpkg.com/@popperjs/core@2" defer></script>
<script src="https://unpkg.com/tippy.js@6" defer></script>
<script src="https://unpkg.com/mark.js@8" defer></script>

<!-- include all js in scripts folder -->


  <script src="/_scripts/anchors.js"></script>

  <script src="/_scripts/brain_logo.js"></script>

  <script src="/_scripts/dark-mode.js"></script>

  <script src="/_scripts/fetch-tags.js"></script>

  <script src="/_scripts/search.js"></script>

  <script src="/_scripts/site-search.js"></script>

  <script src="/_scripts/tooltip.js"></script>


</head>

  <body>
    







<header class="background" style="--image: url('/images/background.jpg')" data-dark="true">
  <a href="/" class="home">
    <span class="brainContainer"></span>
    
      <span class="title" data-tooltip="Home">
        
          <span>Explainable Machine Learning Group</span>
        
        
          <span>EML</span>
        
      </span>
    
  </a>

  <input class="nav-toggle" type="checkbox" aria-label="show/hide nav">

  <nav>
    
    
      
        <a href="/research/" data-tooltip="High-level research overview">
          Research
        </a>
      
    
      
        <a href="/publications/" data-tooltip="Published works">
          Publications
        </a>
      
    
      
        <a href="/projects/" data-tooltip="Software, datasets, and more">
          Projects
        </a>
      
    
      
        <a href="/team/" data-tooltip="About our team">
          Team
        </a>
      
    
      
        <a href="/contact/" data-tooltip="Email, address, and location">
          Contact
        </a>
      
    
  </nav>
</header>

    <main>
      <!--
  modify main content of page:
  - add section breaks
  - attach section properties
  - wrap each table in div to allow for scrolling
  - filter out blank sections
-->








  
  
  

  <section class="background" data-size="page">
    <h1 id="research">
<i class="icon fa-solid fa-microscope"></i>Research</h1>

<p>Research is most fruitful (and most fun) at the intersection of different fields of research. At the core of our research is the development of methods that
provide <em>explanations</em> for complex machine learning models, as well as the development of <em>inherently interpretable</em> machine learning models.
We are particularly interested in getting a better understanding of how information is encoded and propagated <em>inside</em> a neural network, nowadays coined <em>mechanistic interpretability</em>, or to inform <em>inherently interpretable</em> and <em>neuro-symbolic</em>
architecture designs.
Our research spans from foundations to applications in practice. One focus lies on <em>extracting new scientific insights</em> from models that often surpass human experts through methods for Interpretability of Machine Learning models.
In molecular biology we for example use explanations of models to get new knowledge about the complex genomic mechanisms driving cancer or aging.</p>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<div class="feature">
  <a class="feature-image" aria-label="Inherently Interpretable decision-making">
    <img src="/images/cave.png" loading="lazy" alt="Inherently Interpretable decision-making" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  <div class="feature-text">
    
      <p class="feature-title">Inherently Interpretable decision-making</p>
    
    
<p><em>Inherently interpretable models</em> are key to ensure explanations that are <em>faithful</em> to the actual model decisions. Such Explanations that reveal the true decision-making of the model are key in high-stakes settings in practice,
for example in biomedical imaging – e.g., predicting cancer risk or events based on radiological images, tissue stains, or skin screens.
We are interested in developing new methodology that yields both highly performant but still inherently interpretable models, ensure that explanations are correctly grounded in the input image, as well as models that are inherently interpretabl while emitting other properties such as OOD robustness.
Most recently, we have been interested in enabling inherent explanations on downstream tasks for foundation models such as CLIP, and making multimodal model reasoning, such as of Vision Language Models, more transparent.</p>


  </div>
</div>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<div class="feature">
  <a class="feature-image" aria-label="Global Explanations">
    <img src="/images/explainn.png" loading="lazy" alt="Global Explanations" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  <div class="feature-text">
    
      <p class="feature-title">Global Explanations</p>
    
    
<p><em>Post-hoc</em> explainability methods aim to provide explanations of what an already trained model, usually a neural network, learned and how it uses this information to arrive at a prediction.
While not making a black box model fully transparent, by providing a glimpse into the black box these methods have the great advantage that they do not compromise performance, which is a desiderata for a neural network in practice. 
A majority of post-hoc explanation approaches, however, focus on <em>instance-specific explanations</em>, giving insights into why a decision was made for a particular instance (e.g., saliency maps, GradCam, Integrated Gradients,
LIME, …).
We are interested in <em>faithful explanations</em> that accuratly describe the general information encoded for example by a group of neurons and discovering such feature-encoding sub-networks (<em>mechanistic circuits</em>).
A recent focus of the group has been to also develop <em>theoretical guarantees</em>, e.g. on robustness or uncertainty, for such circuitry</p>


  </div>
</div>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<div class="feature">
  <a class="feature-image" aria-label="Neuro-symbolic architectures">
    <img src="/images/binaps.png" loading="lazy" alt="Neuro-symbolic architectures" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  <div class="feature-text">
    
      <p class="feature-title">Neuro-symbolic architectures</p>
    
    
<p><em>Neuro-symbolic approaches</em> to machine learning promise full interpretability through <em>symbolic encodings</em> in a neural network that can be parsed and understood by a human domain expert. While highly interesting, these approaches remain largely theoretical, as such architectures are hard to define and even harder to optimize. We focus on recent advances for <em>binarized and ternarized neural network architectures</em> from different fields, such as ML for embedded devices, and how we can transfer these to neuro-symbolic learning. In several biological applications as well as settings with transactional data, such architectures already provide a fully interpretable approach that allows to derive new insights directly from the network.</p>


  </div>
</div>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->

<div class="feature">
  <a class="feature-image" aria-label="Sparse Neural Networks">
    <img src="/images/pruning.png" loading="lazy" alt="Sparse Neural Networks" onerror="this.src = '/images/fallback.svg'; this.onerror = null;">
  </a>
  <div class="feature-text">
    
      <p class="feature-title">Sparse Neural Networks</p>
    
    
<p><em>Sparse neural networks</em> not only provide more resource-efficient alternatives to their dense counterparts at same performance, captured by the <em>Lottery Ticket Hypothesis</em>, but also represent useful information in their network topology compared to the standard fully connected layers in dense networks.
We focus on the question of how we can prune networks to <em>extreme sparsity</em>, and how such sparse neural networks can then yield more explainable decision-making.</p>


  </div>
</div>
  </section>

  
  
  

  <section class="background" data-size="page">
    <!--
  background: ;
  dark: ;
  size: ;
-->
  </section>


    </main>
    


<footer class="background" style="--image: url('/images/background.jpg')" data-dark="true" data-size="wide">
  <!--
    <div>
      Extra details like contact info or address
    </div>
  -->

  <div>
    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://scholar.google.com/citations?user=Gfc2NA4AAAAJ" data-tooltip="Google Scholar" data-style="bare" aria-label="Google Scholar">
      <i class="icon fa-brands fa-google"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://github.com/ExplainableMachines" data-tooltip="GitHub" data-style="bare" aria-label="GitHub">
      <i class="icon fa-brands fa-github"></i>
      
    </a>
  </div>


    
      
      
      



  <div class="button-wrapper">
    <a class="button" href="https://twitter.com/JonasFischerML" data-tooltip="Twitter" data-style="bare" aria-label="Twitter">
      <i class="icon fa-brands fa-twitter"></i>
      
    </a>
  </div>


    
  </div>

  <div>
    © 2026
    Explainable Machine Learning Group
      |   Built with
    <a href="https://github.com/greenelab/lab-website-template">
      Lab Website Template
    </a>
  </div>

  <input type="checkbox" class="dark-toggle" data-tooltip="Dark mode" aria-label="toggle dark mode" oninput="onDarkToggleChange(event)">
</footer>

  </body>
</html>
